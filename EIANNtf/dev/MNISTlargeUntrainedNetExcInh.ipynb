{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wF5wszaj97Y"
      },
      "source": [
        "# MNIST Large Untrained Net Exc Inh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04QgGZc9bF5D"
      },
      "source": [
        "Derived from https://github.com/tensorflow/docs/blob/master/site/en/tutorials/quickstart/beginner.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "0trJmd6DjqBZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "125036ea-ac5c-4ff4-c7a7-57eb429a0275"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.8.2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "from keras import backend as K\n",
        "#print(\"keras version:\",tf.keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "np.set_printoptions(threshold=sys.maxsize)"
      ],
      "metadata": {
        "id": "36nIv8fWBeuV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inlineImplementation = False\t#orig: True #True: excitatory/inhibitory neurons are on same sublayer, False: add inhibitory neurons to separate preceding sublayer\n",
        "if(inlineImplementation):\n",
        "    positiveWeightImplementation = False\t#orig: True #optional\n",
        "    if(not positiveWeightImplementation):\n",
        "        integrateWeights = True    #orig: False #optional\n",
        "        if(integrateWeights):\n",
        "            integrateWeights1 = False    #explicitly declare E/I neurons\n",
        "            integrateWeights2 = True    #implicitly declare E/I neurons \n",
        "            integrateWeightsInitialiseZero = False   #miscellaneous training performance improvement (single EI layer only)\n",
        "else:\n",
        "    positiveWeightImplementation = False    #False: only current coded implementation\n",
        "\n",
        "inputLayerExcitatoryOnly = True #True: only current coded implementation\n",
        "\n",
        "generateUntrainedNetwork = False\n",
        "if(generateUntrainedNetwork):\n",
        "    #only train the last layer\n",
        "    numberOfHiddenLayers = 2    #default: 2    #if 0 then useSVM=True\n",
        "else:\n",
        "    numberOfHiddenLayers = 2 #default: 2\n",
        "\n",
        "if(numberOfHiddenLayers > 1):\n",
        "    addSkipLayers = False   #optional\n",
        "else:\n",
        "    addSkipLayers = False   #mandatory\n",
        "\n",
        "layerSizeBase = 128  #default: 128\n",
        "\n",
        "batch_size = 64 #default: 64\n",
        "epochs = 100  #1  #5\n",
        "\n",
        "debugNoEIneurons = False\n",
        "debugPreTrainWeights = True\n",
        "debugPreTrainOutputs = True\n",
        "debugPostTrainWeights = True\n",
        "debugPostTrainOutputs = True"
      ],
      "metadata": {
        "id": "cq_TphnxNEe8"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGfQXr--YeL7"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "7FP5258xjs-v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ef7186d-a27f-4909-9764-15d3b0456aae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train.shape =  (60000, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "print(\"x_train.shape = \", x_train.shape)\n",
        "\n",
        "input_shape = (28, 28)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPZ68wASog_I"
      },
      "source": [
        "## Define model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10"
      ],
      "metadata": {
        "id": "29RfPJhqwngo"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EIactivation(x):\n",
        "    return K.maximum(x, 0)  #ReLU\n",
        "    \n",
        "def EIactivationExcitatory(x):\n",
        "    if(inlineImplementation):\n",
        "        if(positiveWeightImplementation):\n",
        "            return K.maximum(x, 0)  #ReLU\n",
        "        else:\n",
        "             print(\"EIactivationExcitatory error: requires positiveWeightImplementation\")      \n",
        "    else:\n",
        "        print(\"EIactivationExcitatory error: requires inlineImplementation\")\n",
        "\n",
        "def EIactivationInhibitory(x):\n",
        "    if(inlineImplementation):\n",
        "        if(positiveWeightImplementation):\n",
        "            return -(K.maximum(x, 0))   #ReLU with negative output\n",
        "        else:\n",
        "             print(\"EIactivationInhibitory error: requires positiveWeightImplementation\")      \n",
        "    else:\n",
        "        print(\"inlineImplementation error: requires inlineImplementation\")\n",
        "\n",
        "def EIweightInitializer(shape, dtype=None):\n",
        "    if(inlineImplementation):\n",
        "        if(positiveWeightImplementation):\n",
        "            w = tf.math.abs(tf.random.normal(shape, dtype=dtype))\n",
        "        else:\n",
        "            if(integrateWeights):\n",
        "                if(integrateWeightsInitialiseZero):\n",
        "                    w = tf.random.normal(shape, dtype=dtype)\n",
        "                    #w = tf.zeros(shape, dtype=dtype)    #tf.math.abs(tf.random.normal(shape, dtype=dtype))\n",
        "                else:\n",
        "                    w = tf.math.abs(tf.random.normal(shape, dtype=dtype))\n",
        "                    wEIsize = w.shape[0]//2\n",
        "                    wSignE = tf.ones([wEIsize, w.shape[1]])\n",
        "                    wSignI = tf.ones([wEIsize, w.shape[1]])\n",
        "                    wSignI = tf.multiply(wSignI, -1)\n",
        "                    wSign = tf.concat([wSignE, wSignI], axis=0)\n",
        "                    w = tf.multiply(w, wSign)\n",
        "            else:\n",
        "                print(\"EIweightInitializer error: requires !positiveWeightImplementation:integrateWeights\")\n",
        "    else:\n",
        "        print(\"EIweightInitializer error: requires inlineImplementation\")\n",
        "\n",
        "    return w\n",
        "\n",
        "def EIweightInitializerExcitatory(shape, dtype=None):\n",
        "    if(positiveWeightImplementation):\n",
        "        print(\"EIweightInitializerExcitatory error: requires !positiveWeightImplementation\")\n",
        "    else:\n",
        "        return tf.math.abs(tf.random.normal(shape, dtype=dtype))\n",
        "\n",
        "def EIweightInitializerInhibitory(shape, dtype=None):\n",
        "    if(positiveWeightImplementation):\n",
        "        print(\"EIweightInitializerExcitatory error: requires !positiveWeightImplementation\")\n",
        "    else:\n",
        "        return tf.math.negative(tf.math.abs(tf.random.normal(shape, dtype=dtype)))\n",
        "\n",
        "class negative(tf.keras.constraints.Constraint):\n",
        "    #based on https://www.tensorflow.org/api_docs/python/tf/keras/constraints/Constraint\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def __call__(self, w):\n",
        "        return w * tf.cast(tf.math.less_equal(w, 0.), w.dtype)\n",
        "\n",
        "class positiveOrNegative(tf.keras.constraints.Constraint):\n",
        "    #based on https://www.tensorflow.org/api_docs/python/tf/keras/constraints/Constraint\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def __call__(self, w):\n",
        "        w_shape = w.shape\n",
        "        wEIsize = w.shape[0]//2\n",
        "        wE = w[0:wEIsize]\n",
        "        wI = w[wEIsize:]\n",
        "        wEcheck = tf.greater_equal(wE, 0)\n",
        "        wIcheck = tf.less_equal(wI, 0)\n",
        "        wEcheck = tf.cast(wEcheck, tf.float32)\n",
        "        wIcheck = tf.cast(wIcheck, tf.float32)\n",
        "        wE = tf.multiply(wE, wEcheck)\n",
        "        wI = tf.multiply(wI, wIcheck)\n",
        "        w = tf.concat([wE, wI], axis=0)\n",
        "        return w\n"
      ],
      "metadata": {
        "id": "bcEe88PrSf5h"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if(inlineImplementation):\n",
        "    if(positiveWeightImplementation):\n",
        "        EIweightConstraint = tf.keras.constraints.non_neg()\n",
        "        constrainBiases = True   #ensure positive biases also\n",
        "        if(constrainBiases):\n",
        "            EIbiasConstraint = tf.keras.constraints.non_neg()\n",
        "            constrainBiasesLastLayer = False\n",
        "            if(constrainBiasesLastLayer):\n",
        "                EIbiasConstraintLastLayer = tf.keras.constraints.non_neg()\n",
        "            else:\n",
        "                EIbiasConstraintLastLayer = None\n",
        "        else:\n",
        "            EIbiasConstraint = None\n",
        "            EIbiasConstraintLastLayer = None\n",
        "        EIweightConstraintLastLayer = EIweightConstraint\n",
        "    else:\n",
        "        if(integrateWeights):\n",
        "            EIweightConstraint = positiveOrNegative()\n",
        "            EIbiasConstraint = None\n",
        "            EIweightConstraintLastLayer = None\n",
        "            EIbiasConstraintLastLayer = None\n",
        "        else:\n",
        "            EIweightConstraintPositive = tf.keras.constraints.non_neg()\n",
        "            EIweightConstraintNegative = negative()\n",
        "            constrainBiases = False\n",
        "            if(constrainBiases):\n",
        "                EIbiasConstraintPositive = tf.keras.constraints.non_neg()\n",
        "                EIbiasConstraintNegative = negative()\n",
        "            else:\n",
        "                EIbiasConstraintPositive = None\n",
        "                EIbiasConstraintNegative = None\n",
        "            EIweightConstraintLastLayer = None\n",
        "            EIbiasConstraintLastLayer = None\n",
        "else:\n",
        "    EIweightConstraintPositive = tf.keras.constraints.non_neg()\n",
        "    EIweightConstraintNegative = negative()\n",
        "    constrainBiases = False\n",
        "    if(constrainBiases):\n",
        "        EIbiasConstraintPositive = tf.keras.constraints.non_neg()\n",
        "        EIbiasConstraintNegative = negative()\n",
        "    else:\n",
        "        EIbiasConstraintPositive = None\n",
        "        EIbiasConstraintNegative = None\n",
        "    EIweightConstraintLastLayer = None\n",
        "    EIbiasConstraintLastLayer = None  \n",
        "\n",
        "if(generateUntrainedNetwork):\n",
        "    #only train the last layer\n",
        "    generateLargeNetwork = True\n",
        "else:\n",
        "    generateLargeNetwork = False\n",
        "\n",
        "\n",
        "if(generateLargeNetwork):\n",
        "    generateLargeNetworkRatio = 50\n",
        "    layerRatio = generateLargeNetworkRatio\n",
        "else:\n",
        "    layerRatio = 1  #10 #1\n",
        "\n",
        "def createEIlayer(layerIndex, h0, firstLayer=False):\n",
        "    if(debugNoEIneurons):\n",
        "        h1 = tf.keras.layers.Dense(layerSizeBase*layerRatio)(h0)\n",
        "        h1 = tf.keras.layers.ReLU()(h1)\n",
        "    else:\n",
        "        if(inlineImplementation):\n",
        "            if(positiveWeightImplementation):\n",
        "                h1E = tf.keras.layers.Dense(layerSizeBase*layerRatio, kernel_initializer=EIweightInitializerExcitatory, kernel_constraint=EIweightConstraint, bias_constraint=EIbiasConstraint)(h0)\n",
        "                h1I = tf.keras.layers.Dense(layerSizeBase*layerRatio, kernel_initializer=EIweightInitializerInhibitory, kernel_constraint=EIweightConstraint, bias_constraint=EIbiasConstraint)(h0)\n",
        "                h1E = tf.keras.layers.Activation(EIactivationExcitatory)(h1E)\n",
        "                h1I = tf.keras.layers.Activation(EIactivationInhibitory)(h1I)\n",
        "                h1 = tf.keras.layers.Concatenate()([h1E, h1I])\n",
        "            else:\n",
        "                if(integrateWeights):\n",
        "                    if(integrateWeights1):\n",
        "                        if(firstLayer):\n",
        "                            h1E = tf.keras.layers.Dense(layerSizeBase*layerRatio)(h0)   #excitatory neuron inputs\n",
        "                            h1I = tf.keras.layers.Dense(layerSizeBase*layerRatio)(h0)   #inhibitory neuron inputs             \n",
        "                        else:\n",
        "                            h1E = tf.keras.layers.Dense(layerSizeBase*layerRatio, kernel_initializer=EIweightInitializer, kernel_constraint=EIweightConstraint, bias_constraint=EIbiasConstraint)(h0)   #excitatory neuron inputs\n",
        "                            h1I = tf.keras.layers.Dense(layerSizeBase*layerRatio, kernel_initializer=EIweightInitializer, kernel_constraint=EIweightConstraint, bias_constraint=EIbiasConstraint)(h0)   #inhibitory neuron inputs  \n",
        "                        h1E = tf.keras.layers.Activation(EIactivation)(h1E)\n",
        "                        h1I = tf.keras.layers.Activation(EIactivation)(h1I)\n",
        "                        h1 = tf.keras.layers.Concatenate()([h1E, h1I])\n",
        "                    elif(integrateWeights2):\n",
        "                        if(firstLayer):\n",
        "                            h1 = tf.keras.layers.Dense(layerSizeBase*layerRatio*4)(h0)   #excitatory neuron inputs\n",
        "                        else:\n",
        "                            h1 = tf.keras.layers.Dense(layerSizeBase*layerRatio*2, kernel_initializer=EIweightInitializer, kernel_constraint=EIweightConstraint, bias_constraint=EIbiasConstraint)(h0)   #excitatory neuron inputs\n",
        "                        h1 = tf.keras.layers.Activation(EIactivation)(h1) #ReLU\n",
        "                else:\n",
        "                    if(firstLayer):\n",
        "                        h1E = tf.keras.layers.Dense(layerSizeBase*layerRatio)(h0)   #excitatory neuron inputs\n",
        "                        h1I = tf.keras.layers.Dense(layerSizeBase*layerRatio)(h0)   #inhibitory neuron inputs\n",
        "                    else:\n",
        "                        h0E, h0I = h0\n",
        "                        h1Ee = tf.keras.layers.Dense(layerSizeBase*layerRatio, kernel_initializer=EIweightInitializerExcitatory, kernel_constraint=EIweightConstraintPositive, bias_constraint=EIbiasConstraintPositive)(h0E) #excitatory neuron excitatory inputs\n",
        "                        h1Ei = tf.keras.layers.Dense(layerSizeBase*layerRatio, kernel_initializer=EIweightInitializerInhibitory, kernel_constraint=EIweightConstraintNegative, bias_constraint=EIbiasConstraintNegative)(h0I) #excitatory neuron inhibitory inputs\n",
        "                        h1Ie = tf.keras.layers.Dense(layerSizeBase*layerRatio, kernel_initializer=EIweightInitializerExcitatory, kernel_constraint=EIweightConstraintPositive, bias_constraint=EIbiasConstraintPositive)(h0E) #inhibitory neuron excitatory inputs\n",
        "                        h1Ii = tf.keras.layers.Dense(layerSizeBase*layerRatio, kernel_initializer=EIweightInitializerInhibitory, kernel_constraint=EIweightConstraintNegative, bias_constraint=EIbiasConstraintNegative)(h0I) #inhibitory neuron inhibitory inputs\n",
        "                        h1E = tf.keras.layers.Add()([h1Ee, h1Ei])\n",
        "                        h1I = tf.keras.layers.Add()([h1Ie, h1Ii])\n",
        "                    h1E = tf.keras.layers.Activation(EIactivation)(h1E)\n",
        "                    h1I = tf.keras.layers.Activation(EIactivation)(h1I)\n",
        "                    h1 = (h1E, h1I)\n",
        "        else:\n",
        "            h1I = tf.keras.layers.Dense(layerSizeBase*layerRatio, kernel_initializer=EIweightInitializerExcitatory, kernel_constraint=EIweightConstraintPositive, bias_constraint=EIbiasConstraintPositive)(h0) #inhibitory interneuron (excitatory inputs)\n",
        "            h1I = tf.keras.layers.Activation(EIactivation)(h1I) #not required\n",
        "            h1I = h1I*calculateInhibitoryNeuronNormalisationFactor(h0, h1I)\n",
        "            h1Ee = tf.keras.layers.Dense(layerSizeBase*layerRatio, kernel_initializer=EIweightInitializerExcitatory, kernel_constraint=EIweightConstraintPositive, bias_constraint=EIbiasConstraintPositive)(h0) #excitatory neuron excitatory inputs\n",
        "            h1Ei = tf.keras.layers.Dense(layerSizeBase*layerRatio, kernel_initializer=EIweightInitializerInhibitory, kernel_constraint=EIweightConstraintNegative, bias_constraint=EIbiasConstraintNegative)(h1I) #excitatory neuron inhibitory inputs\n",
        "            h1E = tf.keras.layers.Add()([h1Ee, h1Ei])\n",
        "            h1E = tf.keras.layers.Activation(EIactivation)(h1E)\n",
        "            h1 = h1E\n",
        "    return h1\n",
        "\n",
        "def calculateInhibitoryNeuronNormalisationFactor(h0, h1I):\n",
        "    h1InormalisationFactor = tf.reduce_mean(h0)/tf.reduce_mean(h1I)\n",
        "    return h1InormalisationFactor\n",
        "\n",
        "def concatEIneurons(h):\n",
        "    if(inlineImplementation):\n",
        "        if(positiveWeightImplementation):\n",
        "            return h\n",
        "        else: \n",
        "            if(integrateWeights):\n",
        "                pass\n",
        "            else:\n",
        "                hE, hI = h\n",
        "                h = tf.keras.layers.Concatenate()([hE, hI])\n",
        "            return h\n",
        "    else:\n",
        "        return h"
      ],
      "metadata": {
        "id": "msg_fj8mwhe0"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "h3IKyzTCDNGo"
      },
      "outputs": [],
      "source": [
        "x = tf.keras.layers.Input(shape=input_shape)\n",
        "h0 = tf.keras.layers.Flatten()(x)\n",
        "hLast = h0\n",
        "\n",
        "if(numberOfHiddenLayers >= 1):\n",
        "    h1 = createEIlayer(1, h0, firstLayer=True)\n",
        "    hLast = h1\n",
        "if(numberOfHiddenLayers >= 2):\n",
        "    h2 = createEIlayer(2, h1)\n",
        "    hLast = h2\n",
        "if(numberOfHiddenLayers >= 3):\n",
        "    h3 = createEIlayer(3, h2)\n",
        "    hLast = h3\n",
        "if(numberOfHiddenLayers >= 4):\n",
        "    h4 = createEIlayer(4, h3)\n",
        "    hLast = h4\n",
        "\n",
        "if(addSkipLayers):\n",
        "    mList = []\n",
        "    if(numberOfHiddenLayers >= 1):\n",
        "        m1 = tf.keras.layers.Flatten()(concatEIneurons(h1))\n",
        "        mList.append(m1)\n",
        "    if(numberOfHiddenLayers >= 2):\n",
        "        m2 = tf.keras.layers.Flatten()(concatEIneurons(h2))\n",
        "        mList.append(m2)\n",
        "    if(numberOfHiddenLayers >= 3):\n",
        "        m3 = tf.keras.layers.Flatten()(concatEIneurons(h3))\n",
        "        mList.append(m3)\n",
        "    if(numberOfHiddenLayers >= 4):\n",
        "        m4 = tf.keras.layers.Flatten()(concatEIneurons(h4))\n",
        "        mList.append(m4)\n",
        "    hLast = tf.keras.layers.concatenate(mList)\n",
        "else:\n",
        "    hLast = concatEIneurons(hLast)\n",
        "\n",
        "if(generateUntrainedNetwork):\n",
        "    hLast = tf.keras.layers.Lambda(lambda x: tf.keras.backend.stop_gradient(x))(hLast)\n",
        "\n",
        "y = tf.keras.layers.Dense(num_classes, activation='softmax', kernel_constraint=EIweightConstraintLastLayer, bias_constraint=EIbiasConstraintLastLayer)(hLast)\n",
        "model = tf.keras.Model(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "RSkzdv8MD0tT"
      },
      "outputs": [],
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "9foNKHzTD2Vo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c4fbf49-03fd-4b0f-e942-aa353b789b11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_15\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 28, 28)]     0           []                               \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 784)          0           ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 128)          100480      ['flatten_2[0][0]']              \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 128)          0           ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean (TFOpLambd  ()                  0           ['flatten_2[0][0]']              \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_1 (TFOpLam  ()                  0           ['activation[0][0]']             \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " tf.math.truediv (TFOpLambda)   ()                   0           ['tf.math.reduce_mean[0][0]',    \n",
            "                                                                  'tf.math.reduce_mean_1[0][0]']  \n",
            "                                                                                                  \n",
            " tf.math.multiply (TFOpLambda)  (None, 128)          0           ['activation[0][0]',             \n",
            "                                                                  'tf.math.truediv[0][0]']        \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 128)          100480      ['flatten_2[0][0]']              \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 128)          16512       ['tf.math.multiply[0][0]']       \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 128)          0           ['dense_5[0][0]',                \n",
            "                                                                  'dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 128)          0           ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 128)          16512       ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 128)          0           ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_2 (TFOpLam  ()                  0           ['activation_1[0][0]']           \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_3 (TFOpLam  ()                  0           ['activation_2[0][0]']           \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " tf.math.truediv_1 (TFOpLambda)  ()                  0           ['tf.math.reduce_mean_2[0][0]',  \n",
            "                                                                  'tf.math.reduce_mean_3[0][0]']  \n",
            "                                                                                                  \n",
            " tf.math.multiply_1 (TFOpLambda  (None, 128)         0           ['activation_2[0][0]',           \n",
            " )                                                                'tf.math.truediv_1[0][0]']      \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 128)          16512       ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 128)          16512       ['tf.math.multiply_1[0][0]']     \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 128)          0           ['dense_8[0][0]',                \n",
            "                                                                  'dense_9[0][0]']                \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 128)          0           ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 10)           1290        ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 268,298\n",
            "Trainable params: 268,298\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
        "    #temp: model.compile(optimizer=tf.keras.optimizers.RMSprop(epsilon=1e-08), loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "print(model.summary())\n",
        "#printModelSummary(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if(debugPreTrainWeights):\n",
        "    testwritefile = open('weightsPreTrain.txt', 'w')\n",
        "    for layerIndex, layer in enumerate(model.layers):\n",
        "        heading = \"\\n\" + \"layer = \" + str(layerIndex) + \"\\n\"\n",
        "        testwritefile.write(heading)\n",
        "        #print(heading)\n",
        "        weights = layer.get_weights()\n",
        "        #print(weights)\n",
        "        weightsS =  str(weights)\n",
        "        testwritefile.write(weightsS)\n",
        "    testwritefile.close()"
      ],
      "metadata": {
        "id": "FAQw0rvCd4Wt"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if(debugPreTrainOutputs):\n",
        "    testwritefile = open('outputPreTrain.txt', 'w')\n",
        "    xTrainFirstSample = np.expand_dims(x_train[0], axis=0)\n",
        "    for layerIndex, layer in enumerate(model.layers):\n",
        "        heading = \"\\n\" + \"layer = \" + str(layerIndex) + \"\\n\"\n",
        "        #print(heading)\n",
        "        testwritefile.write(heading)\n",
        "        func = K.function([model.get_layer(index=0).input], layer.output)\n",
        "        layerOutput = func([xTrainFirstSample])  # input_data is a numpy array\n",
        "        #print(\"layerOutput.shape = \", layerOutput.shape)\n",
        "        layerOutputS =  str(layerOutput)\n",
        "        testwritefile.write(layerOutputS)\n",
        "    testwritefile.close()"
      ],
      "metadata": {
        "id": "o8FNyXG9Pl-G"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model"
      ],
      "metadata": {
        "id": "5lJZhEkFYCtL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "y7suUbJXVLqP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "339ed1ea-8bde-428e-ef5c-0f033f6fd35c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "938/938 [==============================] - 4s 3ms/step - loss: 11.6806 - accuracy: 0.7004\n",
            "Epoch 2/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 4.3761 - accuracy: 0.8207\n",
            "Epoch 3/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 3.7554 - accuracy: 0.8480\n",
            "Epoch 4/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 3.1072 - accuracy: 0.8628\n",
            "Epoch 5/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 2.9342 - accuracy: 0.8665\n",
            "Epoch 6/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 2.4386 - accuracy: 0.8770\n",
            "Epoch 7/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 2.3769 - accuracy: 0.8755\n",
            "Epoch 8/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 1.8001 - accuracy: 0.8911\n",
            "Epoch 9/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 1.5959 - accuracy: 0.8892\n",
            "Epoch 10/100\n",
            "938/938 [==============================] - 3s 4ms/step - loss: 1.2807 - accuracy: 0.8961\n",
            "Epoch 11/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 1.0298 - accuracy: 0.9000\n",
            "Epoch 12/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.8673 - accuracy: 0.9013\n",
            "Epoch 13/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.6584 - accuracy: 0.9087\n",
            "Epoch 14/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.5109 - accuracy: 0.9140\n",
            "Epoch 15/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.4068 - accuracy: 0.9170\n",
            "Epoch 16/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.3208 - accuracy: 0.9237\n",
            "Epoch 17/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2773 - accuracy: 0.9265\n",
            "Epoch 18/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2356 - accuracy: 0.9336\n",
            "Epoch 19/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2233 - accuracy: 0.9352\n",
            "Epoch 20/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2139 - accuracy: 0.9369\n",
            "Epoch 21/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.2056 - accuracy: 0.9387\n",
            "Epoch 22/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1964 - accuracy: 0.9419\n",
            "Epoch 23/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1927 - accuracy: 0.9433\n",
            "Epoch 24/100\n",
            "938/938 [==============================] - 3s 4ms/step - loss: 0.1908 - accuracy: 0.9436\n",
            "Epoch 25/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1919 - accuracy: 0.9436\n",
            "Epoch 26/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1805 - accuracy: 0.9456\n",
            "Epoch 27/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1705 - accuracy: 0.9490\n",
            "Epoch 28/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1815 - accuracy: 0.9459\n",
            "Epoch 29/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1728 - accuracy: 0.9493\n",
            "Epoch 30/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1596 - accuracy: 0.9519\n",
            "Epoch 31/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1663 - accuracy: 0.9501\n",
            "Epoch 32/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1586 - accuracy: 0.9533\n",
            "Epoch 33/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1597 - accuracy: 0.9521\n",
            "Epoch 34/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1514 - accuracy: 0.9547\n",
            "Epoch 35/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1543 - accuracy: 0.9531\n",
            "Epoch 36/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1550 - accuracy: 0.9546\n",
            "Epoch 37/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1502 - accuracy: 0.9558\n",
            "Epoch 38/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1476 - accuracy: 0.9564\n",
            "Epoch 39/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1455 - accuracy: 0.9566\n",
            "Epoch 40/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1432 - accuracy: 0.9559\n",
            "Epoch 41/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1366 - accuracy: 0.9585\n",
            "Epoch 42/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1435 - accuracy: 0.9568\n",
            "Epoch 43/100\n",
            "938/938 [==============================] - 3s 4ms/step - loss: 0.1378 - accuracy: 0.9589\n",
            "Epoch 44/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1347 - accuracy: 0.9592\n",
            "Epoch 45/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1344 - accuracy: 0.9595\n",
            "Epoch 46/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1314 - accuracy: 0.9617\n",
            "Epoch 47/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1373 - accuracy: 0.9582\n",
            "Epoch 48/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1247 - accuracy: 0.9626\n",
            "Epoch 49/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1323 - accuracy: 0.9601\n",
            "Epoch 50/100\n",
            "938/938 [==============================] - 3s 4ms/step - loss: 0.1300 - accuracy: 0.9605\n",
            "Epoch 51/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1258 - accuracy: 0.9612\n",
            "Epoch 52/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1255 - accuracy: 0.9622\n",
            "Epoch 53/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1245 - accuracy: 0.9612\n",
            "Epoch 54/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1206 - accuracy: 0.9635\n",
            "Epoch 55/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1215 - accuracy: 0.9633\n",
            "Epoch 56/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1200 - accuracy: 0.9635\n",
            "Epoch 57/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1219 - accuracy: 0.9639\n",
            "Epoch 58/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1189 - accuracy: 0.9636\n",
            "Epoch 59/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1160 - accuracy: 0.9644\n",
            "Epoch 60/100\n",
            "938/938 [==============================] - 3s 4ms/step - loss: 0.1136 - accuracy: 0.9649\n",
            "Epoch 61/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1153 - accuracy: 0.9655\n",
            "Epoch 62/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1091 - accuracy: 0.9669\n",
            "Epoch 63/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1171 - accuracy: 0.9652\n",
            "Epoch 64/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1123 - accuracy: 0.9656\n",
            "Epoch 65/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1109 - accuracy: 0.9665\n",
            "Epoch 66/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1087 - accuracy: 0.9671\n",
            "Epoch 67/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1075 - accuracy: 0.9671\n",
            "Epoch 68/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1132 - accuracy: 0.9660\n",
            "Epoch 69/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1071 - accuracy: 0.9668\n",
            "Epoch 70/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1059 - accuracy: 0.9677\n",
            "Epoch 71/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1075 - accuracy: 0.9675\n",
            "Epoch 72/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1077 - accuracy: 0.9669\n",
            "Epoch 73/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1016 - accuracy: 0.9693\n",
            "Epoch 74/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1013 - accuracy: 0.9696\n",
            "Epoch 75/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1000 - accuracy: 0.9694\n",
            "Epoch 76/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1060 - accuracy: 0.9680\n",
            "Epoch 77/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1018 - accuracy: 0.9689\n",
            "Epoch 78/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0990 - accuracy: 0.9695\n",
            "Epoch 79/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0940 - accuracy: 0.9710\n",
            "Epoch 80/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0996 - accuracy: 0.9694\n",
            "Epoch 81/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0990 - accuracy: 0.9698\n",
            "Epoch 82/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0969 - accuracy: 0.9704\n",
            "Epoch 83/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0975 - accuracy: 0.9709\n",
            "Epoch 84/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0952 - accuracy: 0.9712\n",
            "Epoch 85/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0933 - accuracy: 0.9713\n",
            "Epoch 86/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0917 - accuracy: 0.9715\n",
            "Epoch 87/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0925 - accuracy: 0.9721\n",
            "Epoch 88/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0913 - accuracy: 0.9718\n",
            "Epoch 89/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0918 - accuracy: 0.9719\n",
            "Epoch 90/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0916 - accuracy: 0.9720\n",
            "Epoch 91/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0901 - accuracy: 0.9718\n",
            "Epoch 92/100\n",
            "938/938 [==============================] - 3s 4ms/step - loss: 0.0895 - accuracy: 0.9726\n",
            "Epoch 93/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0849 - accuracy: 0.9743\n",
            "Epoch 94/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0886 - accuracy: 0.9729\n",
            "Epoch 95/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0857 - accuracy: 0.9736\n",
            "Epoch 96/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0883 - accuracy: 0.9730\n",
            "Epoch 97/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0844 - accuracy: 0.9740\n",
            "Epoch 98/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0843 - accuracy: 0.9732\n",
            "Epoch 99/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0844 - accuracy: 0.9742\n",
            "Epoch 100/100\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0871 - accuracy: 0.9731\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fde0da1ffd0>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if(debugPostTrainWeights):\n",
        "    testwritefile = open('weightsPostTrain.txt', 'w')\n",
        "    for layerIndex, layer in enumerate(model.layers):\n",
        "        heading = \"\\n\" + \"layer = \" + str(layerIndex) + \"\\n\"\n",
        "        testwritefile.write(heading)\n",
        "        #print(heading)\n",
        "        weights = layer.get_weights()\n",
        "        #print(weights)\n",
        "        weightsS =  str(weights)\n",
        "        testwritefile.write(weightsS)\n",
        "    testwritefile.close()"
      ],
      "metadata": {
        "id": "nyUVIVzvNz3o"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if(debugPostTrainOutputs):\n",
        "    testwritefile = open('outputPostTrain.txt', 'w')\n",
        "    xTrainFirstSample = np.expand_dims(x_train[0], axis=0)\n",
        "    for layerIndex, layer in enumerate(model.layers):\n",
        "        heading = \"\\n\" + \"layer = \" + str(layerIndex) + \"\\n\"\n",
        "        #print(heading)\n",
        "        testwritefile.write(heading)\n",
        "        func = K.function([model.get_layer(index=0).input], layer.output)\n",
        "        layerOutput = func([xTrainFirstSample])  # input_data is a numpy array\n",
        "        #print(layerOutput)\n",
        "        layerOutputS = str(layerOutput)  #tf.tensor.toString(layerOutput)    #layerOutput.tostring()\n",
        "        testwritefile.write(layerOutputS)\n",
        "    testwritefile.close()"
      ],
      "metadata": {
        "id": "wkFMMGAV3DvD"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate model"
      ],
      "metadata": {
        "id": "yp1IpSBHYGpV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "F7dTAzgHDUh7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08c77927-aa3b-4699-bf13-1f36aaefecec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 1s - loss: 0.2223 - accuracy: 0.9507 - 662ms/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.22233952581882477, 0.9506999850273132]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "model.evaluate(x_test,  y_test, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "rYb6DrEH0GMv"
      },
      "outputs": [],
      "source": [
        "probability_model = tf.keras.Sequential([\n",
        "  model,\n",
        "  tf.keras.layers.Softmax()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "cnqOZtUp1YR_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2fc4c4a-3585-4720-80b3-85c680c5816b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n",
              "array([[0.08534293, 0.08534293, 0.08534293, 0.08538448, 0.08534293,\n",
              "        0.08534347, 0.08534293, 0.23187144, 0.08534293, 0.085343  ],\n",
              "       [0.0854473 , 0.08621204, 0.23020951, 0.0854473 , 0.0854473 ,\n",
              "        0.0854473 , 0.0854473 , 0.0854473 , 0.0854473 , 0.0854473 ],\n",
              "       [0.08533791, 0.23195069, 0.08533792, 0.08533791, 0.08533791,\n",
              "        0.08533791, 0.08533791, 0.08534594, 0.08533792, 0.08533791],\n",
              "       [0.23196921, 0.08533674, 0.08533674, 0.08533674, 0.08533674,\n",
              "        0.08533674, 0.08533674, 0.08533675, 0.08533674, 0.08533675],\n",
              "       [0.0853378 , 0.0853378 , 0.0853378 , 0.0853378 , 0.23195243,\n",
              "        0.0853378 , 0.0853378 , 0.08533781, 0.0853378 , 0.08534507]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "probability_model(x_test[:5])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}