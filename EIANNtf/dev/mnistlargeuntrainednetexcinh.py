# -*- coding: utf-8 -*-
"""MNISTlargeUntrainedNetExcInh.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pq3mkfiqpF7Cxmik43QaVxJOKt9dXvQs

# MNIST Large Untrained Net Exc Inh

Derived from https://github.com/tensorflow/docs/blob/master/site/en/tutorials/quickstart/beginner.ipynb
"""

import tensorflow as tf
print("TensorFlow version:", tf.__version__)
from keras import backend as K
#print("keras version:",tf.keras.__version__)

"""## Load data"""

mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

"""## Define model"""

def activationExcitatory(x):
    return K.maximum(x, 0)  #ReLU

def activationInhibitory(x):
    return -(K.maximum(x, 0))   #ReLU with negative output

def excitatoryNeuronInitializer(shape, dtype=None):
    return tf.math.abs(tf.random.normal(shape, dtype=dtype))

def inhibitoryNeuronInitializer(shape, dtype=None):
    #return tf.math.negative(tf.math.abs(tf.random.normal(shape, dtype=dtype)))
    return tf.math.abs(tf.random.normal(shape, dtype=dtype))

input_shape = (28, 28)
num_classes = 10

positiveWeightImplementation = True	#mandatory (only implementation)
if(positiveWeightImplementation):
    weightConstraint = tf.keras.constraints.non_neg()
    positiveWeightImplementationBiases = True   #ensure positive biases also
    if(positiveWeightImplementationBiases):
        biasConstraint = tf.keras.constraints.non_neg()
        positiveWeightImplementationBiasesLastLayer = False
        if(positiveWeightImplementationBiasesLastLayer):
            biasConstraintLastLayer = tf.keras.constraints.non_neg()
        else:
            biasConstraintLastLayer = None
    else:
        biasConstraint = None

generateUntrainedNetwork = False
if(generateUntrainedNetwork):
    #only train the last layer
    generateLargeNetwork = True
    numberOfHiddenLayers = 2
else:
    generateLargeNetwork = False
    numberOfHiddenLayers = 2

if(generateLargeNetwork):
    generateLargeNetworkRatio = 50
    layerRatio = generateLargeNetworkRatio
else:
    layerRatio = 1

x = tf.keras.layers.Input(shape=input_shape)
h0 = tf.keras.layers.Flatten()(x)

hLast = h0
if(numberOfHiddenLayers >= 1):
    h1E = tf.keras.layers.Dense(128*layerRatio, kernel_initializer=excitatoryNeuronInitializer, kernel_constraint=weightConstraint, bias_constraint=biasConstraint)(h0)
    h1I = tf.keras.layers.Dense(128*layerRatio, kernel_initializer=inhibitoryNeuronInitializer, kernel_constraint=weightConstraint, bias_constraint=biasConstraint)(h0)
    h1E = tf.keras.layers.Activation(activationExcitatory)(h1E)
    h1I = tf.keras.layers.Activation(activationInhibitory)(h1I)
    h1 = tf.keras.layers.Concatenate()([h1E, h1I])
    hLast = h1
if(numberOfHiddenLayers >= 2):
    h2E = tf.keras.layers.Dense(128*layerRatio, kernel_initializer=excitatoryNeuronInitializer, kernel_constraint=weightConstraint, bias_constraint=biasConstraint)(h1)
    h2I = tf.keras.layers.Dense(128*layerRatio, kernel_initializer=inhibitoryNeuronInitializer, kernel_constraint=weightConstraint, bias_constraint=biasConstraint)(h1)
    h2E = tf.keras.layers.Activation(activationExcitatory)(h2E)
    h2I = tf.keras.layers.Activation(activationInhibitory)(h2I)
    h2 = tf.keras.layers.Concatenate()([h2E, h2I])
    hLast = h2
if(numberOfHiddenLayers >= 3):
    h3E = tf.keras.layers.Dense(128*layerRatio, kernel_initializer=excitatoryNeuronInitializer, kernel_constraint=weightConstraint, bias_constraint=biasConstraint)(h2)
    h3I = tf.keras.layers.Dense(128*layerRatio, kernel_initializer=inhibitoryNeuronInitializer, kernel_constraint=weightConstraint, bias_constraint=biasConstraint)(h2)
    h3E = tf.keras.layers.Activation(activationExcitatory)(h3E)
    h3I = tf.keras.layers.Activation(activationInhibitory)(h3I)
    h3 = tf.keras.layers.Concatenate()([h3E, h3I])
    hLast = h3

if(generateUntrainedNetwork):
    hLast = tf.keras.layers.Lambda(lambda x: tf.keras.backend.stop_gradient(x))(hLast)
y = tf.keras.layers.Dense(num_classes, activation='softmax', kernel_constraint=weightConstraint, bias_constraint=biasConstraintLastLayer)(hLast)
model = tf.keras.Model(x, y)

#print(model.summary())
#model.compile(optimizer=tf.keras.optimizers.RMSprop(epsilon=1e-08), loss='categorical_crossentropy', metrics=['acc'])
#evaluation accuracy: ? (with 1 or 2 hidden layers)

loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

model.compile(optimizer='adam',
              loss=loss_fn,
              metrics=['accuracy'])

print(model.summary())

for i, layer in enumerate(model.layers):
    weights = layer.get_weights()
    #print("weights = ", weights)

"""## Train model"""

model.fit(x_train, y_train, epochs=5)

for i, layer in enumerate(model.layers):
    weights = layer.get_weights()
    #print("weights = ", weights)



"""## Evaluate model"""

model.evaluate(x_test,  y_test, verbose=2)

probability_model = tf.keras.Sequential([
  model,
  tf.keras.layers.Softmax()
])

probability_model(x_test[:5])