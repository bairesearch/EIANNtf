{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CIFAR10 Large Untrained Net CNN Exc Inh"
      ],
      "metadata": {
        "id": "Bejpr3-EW5q6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Derived from  https://keras.io/zh/examples/cifar10_cnn_tfaugment2d/"
      ],
      "metadata": {
        "id": "eTRKkHORa7GM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inlineImplementation = False\t#False: only implementation  #True: excitatory/inhibitory neurons are on same sublayer, False: add inhibitory neurons to separate preceding sublayer\n",
        "if(not inlineImplementation):\n",
        "    positiveWeightImplementation = False    #False: only current coded implementation\n",
        "\n",
        "useSparsity = False\n",
        "if(useSparsity):\n",
        "  sparsityProbabilityOfConnection = 0.1 #1-sparsity\n",
        "#addSkipLayers = False  #skip layers not supported by keras model.add definition format\n",
        "\n",
        "inputLayerExcitatoryOnly = True #True: only current coded implementation\n",
        "\n",
        "generateUntrainedNetwork = False\n",
        "if(generateUntrainedNetwork):\n",
        "    numberOfHiddenLayers = 2  #default = 2, if 0 then useSVM=True\n",
        "    preFinalDenseLayer = False\n",
        "else:\n",
        "    numberOfHiddenLayers = 2  #default = 4, if 0 then useSVM=True\n",
        "    preFinalDenseLayer = False\n",
        "\n",
        "\n",
        "if(numberOfHiddenLayers > 1):\n",
        "    addSkipLayers = False   #optional\n",
        "else:\n",
        "    addSkipLayers = False   #mandatory\n",
        "\n",
        "layerSizeBase = 32  #default: 32\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 100\n",
        "\n",
        "debugNoEIneurons = False\n",
        "debugPreTrainWeights = True\n",
        "debugPreTrainOutputs = True\n",
        "debugPostTrainWeights = True\n",
        "debugPostTrainOutputs = True\n",
        "if(debugNoEIneurons):\n",
        "    numberOfHiddenLayers = 4  #default = 4, if 0 then useSVM=True\n",
        "    preFinalDenseLayer = True  "
      ],
      "metadata": {
        "id": "oEK0GMrRI6mp"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if(generateUntrainedNetwork):\n",
        "    #only train the last layer\n",
        "    generateLargeNetwork = True\n",
        "else:\n",
        "    generateLargeNetwork = False\n",
        "\n",
        "if(generateLargeNetwork):\n",
        "    largeNetworkRatio = 10    #100\n",
        "    generateLargeNetworkExpansion = False\n",
        "    if(generateLargeNetworkExpansion):\n",
        "        generateLargeNetworkRatioExponential = False\n",
        "else:\n",
        "    generateLargeNetworkRatio = False\n",
        "    largeNetworkRatio = 1\n",
        "    \n",
        "def getLayerRatio(layerIndex):\n",
        "    layerRatio = 1\n",
        "    if(generateLargeNetwork):\n",
        "        if(generateLargeNetworkExpansion):\n",
        "            if(generateLargeNetworkRatioExponential):\n",
        "                layerRatio = largeNetworkRatio**layerIndex\n",
        "            else:\n",
        "                layerRatio = largeNetworkRatio * layerIndex\n",
        "        else:\n",
        "            layerRatio = largeNetworkRatio\n",
        "    else:\n",
        "        layerRatio = 1\n",
        "    return int(layerRatio)\n"
      ],
      "metadata": {
        "id": "UEWRDsS0l6FD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def kernelInitializerWithSparsity(shape, dtype=None):\n",
        "    initialisedWeights = tf.random.normal(shape, dtype=dtype) #change to glorot_uniform?\n",
        "    sparsityMatrixMask = tf.random.uniform(shape, minval=0.0, maxval=1.0, dtype=tf.dtypes.float32)\n",
        "    sparsityMatrixMask = tf.math.less(sparsityMatrixMask, sparsityProbabilityOfConnection)\n",
        "    sparsityMatrixMask = tf.cast(sparsityMatrixMask, dtype=tf.dtypes.float32)\n",
        "    initialisedWeights = tf.multiply(initialisedWeights, sparsityMatrixMask)\n",
        "    return initialisedWeights\n",
        "\n",
        "if(useSparsity):\n",
        "     kernelInitializer = kernelInitializerWithSparsity\n",
        "else:\n",
        "    kernelInitializer = 'glorot_uniform'"
      ],
      "metadata": {
        "id": "ka3S7M_QmcQI"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data"
      ],
      "metadata": {
        "id": "J6NtP4obXJAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, Lambda, MaxPooling2D\n",
        "from keras import backend as K\n",
        "import os\n",
        "\n",
        "if K.backend() != 'tensorflow':\n",
        "    raise RuntimeError('This example can only run with the '\n",
        "                       'TensorFlow backend, '\n",
        "                       'because it requires TF-native augmentation APIs')\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "num_classes = 10\n",
        "num_predictions = 20\n",
        "save_dir = '/tmp/saved_models'\n",
        "model_name = 'keras_cifar10_trained_model.h5'\n",
        "\n",
        "# The data, split between train and test sets:\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "input_shape = (x_train.shape[1], x_train.shape[2], x_train.shape[3])\n",
        "print(\"input_shape = \", input_shape)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255"
      ],
      "metadata": {
        "id": "YrF_byEsXR3K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf272a05-a8d2-484d-92b6-d6554e2bbea2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "input_shape =  (32, 32, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define model"
      ],
      "metadata": {
        "id": "CD-57omeXMe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def EIactivation(x):\n",
        "    return K.maximum(x, 0)  #ReLU\n",
        "\n",
        "def EIactivationExcitatory(x):\n",
        "    if(inlineImplementation):\n",
        "        if(positiveWeightImplementation):\n",
        "            return K.maximum(x, 0)  #ReLU\n",
        "        else:\n",
        "             print(\"EIactivationExcitatory error: requires positiveWeightImplementation\")      \n",
        "    else:\n",
        "        print(\"EIactivationExcitatory error: requires inlineImplementation\")\n",
        "\n",
        "def EIactivationInhibitory(x):\n",
        "    if(inlineImplementation):\n",
        "        if(positiveWeightImplementation):\n",
        "            return -(K.maximum(x, 0))   #ReLU with negative output\n",
        "        else:\n",
        "             print(\"EIactivationInhibitory error: requires positiveWeightImplementation\")      \n",
        "    else:\n",
        "        print(\"inlineImplementation error: requires inlineImplementation\")\n",
        "\n",
        "def EIweightInitializer(shape, dtype=None):\n",
        "    if(inlineImplementation):\n",
        "        if(positiveWeightImplementation):\n",
        "            w = tf.math.abs(tf.random.normal(shape, dtype=dtype))\n",
        "        else:\n",
        "            if(integrateWeights):\n",
        "                if(integrateWeightsInitialiseZero):\n",
        "                    w = tf.zeros(shape, dtype=dtype)    #tf.math.abs(tf.random.normal(shape, dtype=dtype))\n",
        "                else:\n",
        "                    #print(\"shape = \", shape)\n",
        "                    w = tf.math.abs(tf.random.normal(shape, dtype=dtype))\n",
        "                    wEIsize = w.shape[2]//2\n",
        "                    wSignE = tf.ones([w.shape[0], w.shape[1], wEIsize, w.shape[3]])\n",
        "                    wSignI = tf.ones([w.shape[0], w.shape[1], wEIsize, w.shape[3]])\n",
        "                    wSignI = tf.multiply(wSignI, -1)\n",
        "                    wSign = tf.concat([wSignE, wSignI], axis=2)\n",
        "                    w = tf.multiply(w, wSign)\n",
        "            else:\n",
        "                print(\"EIweightInitializer error: requires !positiveWeightImplementation:integrateWeights\")\n",
        "    else:\n",
        "        print(\"EIweightInitializer error: requires inlineImplementation\") \n",
        "\n",
        "    return w\n",
        "\n",
        "def EIweightInitializerExcitatory(shape, dtype=None):\n",
        "    if(positiveWeightImplementation):\n",
        "        print(\"EIweightInitializerExcitatory error: requires !positiveWeightImplementation\")\n",
        "    else:\n",
        "        return tf.math.abs(tf.random.normal(shape, dtype=dtype))\n",
        "\n",
        "def EIweightInitializerInhibitory(shape, dtype=None):\n",
        "    if(positiveWeightImplementation):\n",
        "        print(\"EIweightInitializerExcitatory error: requires !positiveWeightImplementation\")\n",
        "    else:\n",
        "        return tf.math.negative(tf.math.abs(tf.random.normal(shape, dtype=dtype)))\n",
        "\n",
        "class negative(tf.keras.constraints.Constraint):\n",
        "    #based on https://www.tensorflow.org/api_docs/python/tf/keras/constraints/Constraint\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def __call__(self, w):\n",
        "        return w * tf.cast(tf.math.less_equal(w, 0.), w.dtype)\n",
        "\n",
        "class positiveOrNegative(tf.keras.constraints.Constraint):\n",
        "    #based on https://www.tensorflow.org/api_docs/python/tf/keras/constraints/Constraint\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def __call__(self, w):\n",
        "        w_shape = w.shape\n",
        "        #print(\"w_shape = \", w_shape)\n",
        "        wEIsize = w.shape[2]//2\n",
        "        wE = w[:, :, 0:wEIsize]\n",
        "        wI = w[:, :, wEIsize:]\n",
        "        wEcheck = tf.greater_equal(wE, 0)\n",
        "        wIcheck = tf.less_equal(wI, 0)\n",
        "        wEcheck = tf.cast(wEcheck, tf.float32)\n",
        "        wIcheck = tf.cast(wIcheck, tf.float32)\n",
        "        wE = tf.multiply(wE, wEcheck)\n",
        "        wI = tf.multiply(wI, wIcheck)\n",
        "        w = tf.concat([wE, wI], axis=2)\n",
        "        return w"
      ],
      "metadata": {
        "id": "BTzqrhsfk7mG"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if(not inlineImplementation):\n",
        "    EIweightConstraintPositive = tf.keras.constraints.non_neg()\n",
        "    EIweightConstraintNegative = negative()\n",
        "    constrainBiases = False\n",
        "    if(constrainBiases):\n",
        "        EIbiasConstraintPositive = tf.keras.constraints.non_neg()\n",
        "        EIbiasConstraintNegative = negative()\n",
        "    else:\n",
        "        EIbiasConstraintPositive = None\n",
        "        EIbiasConstraintNegative = None\n",
        "    EIweightConstraintLastLayer = None\n",
        "    EIbiasConstraintLastLayer = None \n",
        "\n",
        "\n",
        "def createEIlayer(layerIndex, h0, numChannels, previousNumChannels, firstLayer=False, maxpool2d=None, dropout=None):\n",
        "    layerRatio = getLayerRatio(2)\n",
        "    if(debugNoEIneurons):\n",
        "        h1 = tf.keras.layers.Conv2D(numChannels, (3,3), padding='same')(h0)\n",
        "        h1 = tf.keras.layers.ReLU()(h1)\n",
        "        if(maxpool2d is not None):\n",
        "            h1 = tf.keras.layers.MaxPool2D(pool_size=maxpool2d)(h1)\n",
        "        if(dropout is not None):\n",
        "            h1 = tf.keras.layers.Dropout(dropout)(h1)      \n",
        "    else:\n",
        "        if(not inlineImplementation):\n",
        "            h1I = tf.keras.layers.Conv2D(previousNumChannels, (5,5), padding='same', kernel_initializer=EIweightInitializerExcitatory, kernel_constraint=EIweightConstraintPositive, bias_constraint=EIbiasConstraintPositive)(h0) #inhibitory interneuron (excitatory inputs)\n",
        "            h1I = tf.keras.layers.Activation(EIactivation)(h1I) #not required\n",
        "            h1I = h1I*calculateInhibitoryNeuronNormalisationFactor(h0, h1I)\n",
        "            h1Ee = tf.keras.layers.Conv2D(numChannels, (5,5), padding='same', kernel_initializer=EIweightInitializerExcitatory, kernel_constraint=EIweightConstraintPositive, bias_constraint=EIbiasConstraintPositive)(h0) #excitatory neuron excitatory inputs\n",
        "            h1Ei = tf.keras.layers.Conv2D(numChannels, (5,5), padding='same', kernel_initializer=EIweightInitializerInhibitory, kernel_constraint=EIweightConstraintNegative, bias_constraint=EIbiasConstraintNegative)(h1I) #excitatory neuron inhibitory inputs       \n",
        "            h1E = tf.keras.layers.Add()([h1Ee, h1Ei])\n",
        "            h1E = tf.keras.layers.Activation(EIactivation)(h1E)\n",
        "            h1 = h1E\n",
        "    return h1\n",
        "\n",
        "def calculateInhibitoryNeuronNormalisationFactor(h0, h1I):\n",
        "    h1InormalisationFactor = tf.reduce_mean(h0)/tf.reduce_mean(h1I)\n",
        "    return h1InormalisationFactor\n",
        "\n",
        "def concatEIneurons(h):\n",
        "    if(inlineImplementation):\n",
        "        if(positiveWeightImplementation):\n",
        "            return h\n",
        "        else: \n",
        "            if(integrateWeights):\n",
        "                pass\n",
        "            else:\n",
        "                hE, hI = h\n",
        "                h = tf.keras.layers.Concatenate()([hE, hI])\n",
        "            return h\n",
        "    else:\n",
        "        return h\n"
      ],
      "metadata": {
        "id": "rbQehxeOlom9"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.keras.layers.Input(shape=input_shape)\n",
        "h0 = x\n",
        "hLast = h0\n",
        "\n",
        "previousNumChannels = 3\n",
        "if(numberOfHiddenLayers >= 1):\n",
        "    numChannels = layerSizeBase*1*getLayerRatio(1)\n",
        "    h1 = createEIlayer(1, h0, numChannels, previousNumChannels, firstLayer=True)\n",
        "    hLast = h1\n",
        "    previousNumChannels = numChannels\n",
        "if(numberOfHiddenLayers >= 2):\n",
        "    numChannels = layerSizeBase*1*getLayerRatio(2)\n",
        "    h2 = createEIlayer(2, h1, numChannels, previousNumChannels, maxpool2d=(2,2), dropout=0.25)\n",
        "    hLast = h2\n",
        "    previousNumChannels = numChannels\n",
        "if(numberOfHiddenLayers >= 3):\n",
        "    numChannels = layerSizeBase*2*getLayerRatio(3)\n",
        "    h3 = createEIlayer(3, h2, numChannels, previousNumChannels)\n",
        "    hLast = h3\n",
        "    previousNumChannels = numChannels\n",
        "if(numberOfHiddenLayers >= 4):\n",
        "    numChannels = layerSizeBase*2*getLayerRatio(4)\n",
        "    h4 = createEIlayer(4, h3, numChannels, previousNumChannels, maxpool2d=(2,2), dropout=0.25)\n",
        "    hLast = h4\n",
        "    previousNumChannels = numChannels\n",
        "\n",
        "if(addSkipLayers):\n",
        "    mList = []\n",
        "    if(numberOfHiddenLayers >= 1):\n",
        "        m1 = tf.keras.layers.Flatten()(concatEIneurons(h1))\n",
        "        mList.append(m1)\n",
        "    if(numberOfHiddenLayers >= 2):\n",
        "        m2 = tf.keras.layers.Flatten()(concatEIneurons(h2))\n",
        "        mList.append(m2)\n",
        "    if(numberOfHiddenLayers >= 3):\n",
        "        m3 = tf.keras.layers.Flatten()(concatEIneurons(h3))\n",
        "        mList.append(m3)\n",
        "    if(numberOfHiddenLayers >= 4):\n",
        "        m4 = tf.keras.layers.Flatten()(concatEIneurons(h4))\n",
        "        mList.append(m4)\n",
        "    hLast = tf.keras.layers.concatenate(mList)\n",
        "else:\n",
        "    hLast = concatEIneurons(hLast)\n",
        "\n",
        "hLast = tf.keras.layers.Flatten()(hLast)\n",
        "if(preFinalDenseLayer):\n",
        "    numChannels = 512*largeNetworkRatio\n",
        "    hLast = tf.keras.layers.Dense(numChannels, activation='relu', kernel_initializer=kernelInitializer)(hLast)\n",
        "    hLast = tf.keras.layers.Dropout(0.5)(hLast)\n",
        "\n",
        "if(generateUntrainedNetwork):\n",
        "    hLast = tf.keras.layers.Lambda(lambda x: tf.keras.backend.stop_gradient(x))(hLast)\n",
        "\n",
        "y = tf.keras.layers.Dense(num_classes, activation='softmax')(hLast)\n",
        "model = tf.keras.Model(x, y)"
      ],
      "metadata": {
        "id": "lxYn4_sNXSWT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.summary())\n",
        "#printModelSummary(model)\n",
        "\n",
        "opt = tf.keras.optimizers.RMSprop(epsilon=1e-08)\n",
        "\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['acc'])\n",
        "    #orig: model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDgh_ednvcTg",
        "outputId": "44b5d37a-f85f-4223-bf8b-b0cd502f0b7e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 32, 32, 3)    228         ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 32, 32, 3)    0           ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_4 (TFOpLam  ()                  0           ['input_3[0][0]']                \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_5 (TFOpLam  ()                  0           ['activation_4[0][0]']           \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " tf.math.truediv_2 (TFOpLambda)  ()                  0           ['tf.math.reduce_mean_4[0][0]',  \n",
            "                                                                  'tf.math.reduce_mean_5[0][0]']  \n",
            "                                                                                                  \n",
            " tf.math.multiply_2 (TFOpLambda  (None, 32, 32, 3)   0           ['activation_4[0][0]',           \n",
            " )                                                                'tf.math.truediv_2[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 32, 32, 32)   2432        ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 32, 32, 32)   2432        ['tf.math.multiply_2[0][0]']     \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 32, 32, 32)   0           ['conv2d_11[0][0]',              \n",
            "                                                                  'conv2d_12[0][0]']              \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 32, 32, 32)   0           ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 32, 32, 32)   25632       ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 32, 32, 32)   0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_6 (TFOpLam  ()                  0           ['activation_5[0][0]']           \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_7 (TFOpLam  ()                  0           ['activation_6[0][0]']           \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " tf.math.truediv_3 (TFOpLambda)  ()                  0           ['tf.math.reduce_mean_6[0][0]',  \n",
            "                                                                  'tf.math.reduce_mean_7[0][0]']  \n",
            "                                                                                                  \n",
            " tf.math.multiply_3 (TFOpLambda  (None, 32, 32, 32)  0           ['activation_6[0][0]',           \n",
            " )                                                                'tf.math.truediv_3[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 32, 32, 32)   25632       ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 32, 32, 32)   25632       ['tf.math.multiply_3[0][0]']     \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 32, 32, 32)   0           ['conv2d_14[0][0]',              \n",
            "                                                                  'conv2d_15[0][0]']              \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 32, 32, 32)   0           ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 32768)        0           ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 10)           327690      ['flatten_2[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 409,678\n",
            "Trainable params: 409,678\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model"
      ],
      "metadata": {
        "id": "VdTSQEW7XO23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_test, y_test),\n",
        "          shuffle=True)\n",
        "\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "nMOIUGUvXSz3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "651fbdec-9cb0-46d5-b1df-5d91556ee81e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 50.9541 - acc: 0.3061 - val_loss: 1.8179 - val_acc: 0.3640\n",
            "Epoch 2/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.7376 - acc: 0.3911 - val_loss: 1.7893 - val_acc: 0.3691\n",
            "Epoch 3/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6789 - acc: 0.4153 - val_loss: 1.7713 - val_acc: 0.3849\n",
            "Epoch 4/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6457 - acc: 0.4271 - val_loss: 1.7693 - val_acc: 0.3869\n",
            "Epoch 5/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6183 - acc: 0.4386 - val_loss: 1.7852 - val_acc: 0.3949\n",
            "Epoch 6/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5980 - acc: 0.4474 - val_loss: 1.8271 - val_acc: 0.3861\n",
            "Epoch 7/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5818 - acc: 0.4574 - val_loss: 1.8218 - val_acc: 0.3853\n",
            "Epoch 8/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5647 - acc: 0.4646 - val_loss: 1.8188 - val_acc: 0.3963\n",
            "Epoch 9/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5549 - acc: 0.4685 - val_loss: 1.8378 - val_acc: 0.3817\n",
            "Epoch 10/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5412 - acc: 0.4707 - val_loss: 1.8833 - val_acc: 0.3881\n",
            "Epoch 11/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5304 - acc: 0.4761 - val_loss: 1.8876 - val_acc: 0.3997\n",
            "Epoch 12/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5244 - acc: 0.4794 - val_loss: 1.8540 - val_acc: 0.3878\n",
            "Epoch 13/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5143 - acc: 0.4841 - val_loss: 1.8601 - val_acc: 0.3882\n",
            "Epoch 14/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5047 - acc: 0.4869 - val_loss: 1.8800 - val_acc: 0.4041\n",
            "Epoch 15/100\n",
            "1563/1563 [==============================] - 17s 11ms/step - loss: 1.4962 - acc: 0.4880 - val_loss: 1.8826 - val_acc: 0.3974\n",
            "Epoch 16/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4885 - acc: 0.4957 - val_loss: 1.8627 - val_acc: 0.3953\n",
            "Epoch 17/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4838 - acc: 0.4933 - val_loss: 1.9044 - val_acc: 0.3957\n",
            "Epoch 18/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4822 - acc: 0.4971 - val_loss: 1.9147 - val_acc: 0.3857\n",
            "Epoch 19/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4715 - acc: 0.4984 - val_loss: 1.9135 - val_acc: 0.3885\n",
            "Epoch 20/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4675 - acc: 0.5017 - val_loss: 1.9319 - val_acc: 0.3998\n",
            "Epoch 21/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4599 - acc: 0.5045 - val_loss: 1.9386 - val_acc: 0.3870\n",
            "Epoch 22/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4589 - acc: 0.5048 - val_loss: 1.9326 - val_acc: 0.4011\n",
            "Epoch 23/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4552 - acc: 0.5073 - val_loss: 1.9447 - val_acc: 0.3986\n",
            "Epoch 24/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4526 - acc: 0.5079 - val_loss: 1.9102 - val_acc: 0.3952\n",
            "Epoch 25/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4495 - acc: 0.5078 - val_loss: 1.9505 - val_acc: 0.3868\n",
            "Epoch 26/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4448 - acc: 0.5088 - val_loss: 1.9355 - val_acc: 0.3896\n",
            "Epoch 27/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4467 - acc: 0.5088 - val_loss: 1.9673 - val_acc: 0.3985\n",
            "Epoch 28/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4475 - acc: 0.5096 - val_loss: 1.9481 - val_acc: 0.3930\n",
            "Epoch 29/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4487 - acc: 0.5112 - val_loss: 2.0170 - val_acc: 0.3838\n",
            "Epoch 30/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4454 - acc: 0.5098 - val_loss: 1.9498 - val_acc: 0.3872\n",
            "Epoch 31/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4455 - acc: 0.5099 - val_loss: 1.9821 - val_acc: 0.3891\n",
            "Epoch 32/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4461 - acc: 0.5090 - val_loss: 2.0729 - val_acc: 0.3912\n",
            "Epoch 33/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4390 - acc: 0.5094 - val_loss: 1.9299 - val_acc: 0.3878\n",
            "Epoch 34/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4426 - acc: 0.5100 - val_loss: 1.9616 - val_acc: 0.3791\n",
            "Epoch 35/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4403 - acc: 0.5120 - val_loss: 1.9409 - val_acc: 0.3678\n",
            "Epoch 36/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4397 - acc: 0.5107 - val_loss: 2.1111 - val_acc: 0.3658\n",
            "Epoch 37/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4408 - acc: 0.5093 - val_loss: 2.2212 - val_acc: 0.3889\n",
            "Epoch 38/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4439 - acc: 0.5129 - val_loss: 1.9357 - val_acc: 0.3843\n",
            "Epoch 39/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4561 - acc: 0.5081 - val_loss: 1.9351 - val_acc: 0.3737\n",
            "Epoch 40/100\n",
            "1563/1563 [==============================] - 16s 11ms/step - loss: 1.4375 - acc: 0.5104 - val_loss: 2.2614 - val_acc: 0.3847\n",
            "Epoch 41/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4522 - acc: 0.5103 - val_loss: 2.1223 - val_acc: 0.3919\n",
            "Epoch 42/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4433 - acc: 0.5084 - val_loss: 2.0069 - val_acc: 0.3848\n",
            "Epoch 43/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4476 - acc: 0.5137 - val_loss: 2.0213 - val_acc: 0.3860\n",
            "Epoch 44/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4421 - acc: 0.5118 - val_loss: 2.0040 - val_acc: 0.3739\n",
            "Epoch 45/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4536 - acc: 0.5080 - val_loss: 1.9872 - val_acc: 0.3773\n",
            "Epoch 46/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4513 - acc: 0.5099 - val_loss: 2.0563 - val_acc: 0.3473\n",
            "Epoch 47/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4640 - acc: 0.5038 - val_loss: 1.9480 - val_acc: 0.3626\n",
            "Epoch 48/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4514 - acc: 0.5053 - val_loss: 2.1003 - val_acc: 0.3838\n",
            "Epoch 49/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4787 - acc: 0.5046 - val_loss: 2.0639 - val_acc: 0.3923\n",
            "Epoch 50/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4749 - acc: 0.5008 - val_loss: 2.0189 - val_acc: 0.3803\n",
            "Epoch 51/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4627 - acc: 0.5029 - val_loss: 1.9235 - val_acc: 0.3719\n",
            "Epoch 52/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4729 - acc: 0.5024 - val_loss: 2.0530 - val_acc: 0.3858\n",
            "Epoch 53/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4842 - acc: 0.5006 - val_loss: 2.0056 - val_acc: 0.3758\n",
            "Epoch 54/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4879 - acc: 0.5017 - val_loss: 1.8436 - val_acc: 0.3689\n",
            "Epoch 55/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4792 - acc: 0.5011 - val_loss: 2.2877 - val_acc: 0.3898\n",
            "Epoch 56/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4771 - acc: 0.4986 - val_loss: 1.9526 - val_acc: 0.3798\n",
            "Epoch 57/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4936 - acc: 0.4988 - val_loss: 2.0536 - val_acc: 0.3947\n",
            "Epoch 58/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4879 - acc: 0.4948 - val_loss: 2.1961 - val_acc: 0.3786\n",
            "Epoch 59/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5059 - acc: 0.4948 - val_loss: 1.8539 - val_acc: 0.3486\n",
            "Epoch 60/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4940 - acc: 0.4942 - val_loss: 3.2773 - val_acc: 0.3641\n",
            "Epoch 61/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.4983 - acc: 0.4932 - val_loss: 2.1658 - val_acc: 0.3808\n",
            "Epoch 62/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5023 - acc: 0.4928 - val_loss: 1.8628 - val_acc: 0.3703\n",
            "Epoch 63/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5191 - acc: 0.4885 - val_loss: 1.9708 - val_acc: 0.3965\n",
            "Epoch 64/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5220 - acc: 0.4907 - val_loss: 2.0535 - val_acc: 0.3687\n",
            "Epoch 65/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5373 - acc: 0.4887 - val_loss: 1.8570 - val_acc: 0.3885\n",
            "Epoch 66/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5329 - acc: 0.4850 - val_loss: 1.8970 - val_acc: 0.3502\n",
            "Epoch 67/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5384 - acc: 0.4826 - val_loss: 1.8853 - val_acc: 0.3866\n",
            "Epoch 68/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5432 - acc: 0.4799 - val_loss: 2.3378 - val_acc: 0.3476\n",
            "Epoch 69/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5908 - acc: 0.4774 - val_loss: 3.8874 - val_acc: 0.3802\n",
            "Epoch 70/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5642 - acc: 0.4759 - val_loss: 2.1089 - val_acc: 0.3558\n",
            "Epoch 71/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6383 - acc: 0.4728 - val_loss: 1.8627 - val_acc: 0.3613\n",
            "Epoch 72/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5628 - acc: 0.4727 - val_loss: 2.8050 - val_acc: 0.3756\n",
            "Epoch 73/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5761 - acc: 0.4680 - val_loss: 1.9324 - val_acc: 0.3940\n",
            "Epoch 74/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5814 - acc: 0.4682 - val_loss: 1.9893 - val_acc: 0.3898\n",
            "Epoch 75/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5940 - acc: 0.4671 - val_loss: 2.0021 - val_acc: 0.3872\n",
            "Epoch 76/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5976 - acc: 0.4643 - val_loss: 1.8590 - val_acc: 0.3665\n",
            "Epoch 77/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5961 - acc: 0.4626 - val_loss: 2.4644 - val_acc: 0.3804\n",
            "Epoch 78/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6113 - acc: 0.4612 - val_loss: 2.2333 - val_acc: 0.3768\n",
            "Epoch 79/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6305 - acc: 0.4625 - val_loss: 2.2499 - val_acc: 0.3846\n",
            "Epoch 80/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.5988 - acc: 0.4577 - val_loss: 1.8358 - val_acc: 0.3679\n",
            "Epoch 81/100\n",
            "1563/1563 [==============================] - 16s 11ms/step - loss: 1.6779 - acc: 0.4543 - val_loss: 1.9055 - val_acc: 0.3706\n",
            "Epoch 82/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6459 - acc: 0.4559 - val_loss: 2.0552 - val_acc: 0.3734\n",
            "Epoch 83/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.7372 - acc: 0.4512 - val_loss: 2.5868 - val_acc: 0.3877\n",
            "Epoch 84/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6317 - acc: 0.4508 - val_loss: 1.8821 - val_acc: 0.3789\n",
            "Epoch 85/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6356 - acc: 0.4507 - val_loss: 3.2682 - val_acc: 0.3729\n",
            "Epoch 86/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6493 - acc: 0.4507 - val_loss: 1.9027 - val_acc: 0.3384\n",
            "Epoch 87/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6630 - acc: 0.4497 - val_loss: 2.1207 - val_acc: 0.3581\n",
            "Epoch 88/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6938 - acc: 0.4469 - val_loss: 1.9879 - val_acc: 0.3699\n",
            "Epoch 89/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6350 - acc: 0.4488 - val_loss: 2.6341 - val_acc: 0.3778\n",
            "Epoch 90/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6700 - acc: 0.4467 - val_loss: 2.2755 - val_acc: 0.3780\n",
            "Epoch 91/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6406 - acc: 0.4437 - val_loss: 1.8376 - val_acc: 0.3679\n",
            "Epoch 92/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6527 - acc: 0.4450 - val_loss: 2.0636 - val_acc: 0.2684\n",
            "Epoch 93/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6634 - acc: 0.4380 - val_loss: 2.1248 - val_acc: 0.3832\n",
            "Epoch 94/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6723 - acc: 0.4401 - val_loss: 1.8662 - val_acc: 0.3744\n",
            "Epoch 95/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6446 - acc: 0.4407 - val_loss: 1.8622 - val_acc: 0.3400\n",
            "Epoch 96/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6773 - acc: 0.4404 - val_loss: 2.0745 - val_acc: 0.3781\n",
            "Epoch 97/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6766 - acc: 0.4380 - val_loss: 2.9392 - val_acc: 0.3679\n",
            "Epoch 98/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6600 - acc: 0.4310 - val_loss: 1.8297 - val_acc: 0.3624\n",
            "Epoch 99/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6894 - acc: 0.4355 - val_loss: 2.1228 - val_acc: 0.3754\n",
            "Epoch 100/100\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 1.6836 - acc: 0.4345 - val_loss: 2.0350 - val_acc: 0.3621\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 32, 32, 3)    228         ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 32, 32, 3)    0           ['conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_4 (TFOpLam  ()                  0           ['input_3[0][0]']                \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_5 (TFOpLam  ()                  0           ['activation_4[0][0]']           \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " tf.math.truediv_2 (TFOpLambda)  ()                  0           ['tf.math.reduce_mean_4[0][0]',  \n",
            "                                                                  'tf.math.reduce_mean_5[0][0]']  \n",
            "                                                                                                  \n",
            " tf.math.multiply_2 (TFOpLambda  (None, 32, 32, 3)   0           ['activation_4[0][0]',           \n",
            " )                                                                'tf.math.truediv_2[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 32, 32, 32)   2432        ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 32, 32, 32)   2432        ['tf.math.multiply_2[0][0]']     \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 32, 32, 32)   0           ['conv2d_11[0][0]',              \n",
            "                                                                  'conv2d_12[0][0]']              \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 32, 32, 32)   0           ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 32, 32, 32)   25632       ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 32, 32, 32)   0           ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_6 (TFOpLam  ()                  0           ['activation_5[0][0]']           \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_7 (TFOpLam  ()                  0           ['activation_6[0][0]']           \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " tf.math.truediv_3 (TFOpLambda)  ()                  0           ['tf.math.reduce_mean_6[0][0]',  \n",
            "                                                                  'tf.math.reduce_mean_7[0][0]']  \n",
            "                                                                                                  \n",
            " tf.math.multiply_3 (TFOpLambda  (None, 32, 32, 32)  0           ['activation_6[0][0]',           \n",
            " )                                                                'tf.math.truediv_3[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 32, 32, 32)   25632       ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 32, 32, 32)   25632       ['tf.math.multiply_3[0][0]']     \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 32, 32, 32)   0           ['conv2d_14[0][0]',              \n",
            "                                                                  'conv2d_15[0][0]']              \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 32, 32, 32)   0           ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 32768)        0           ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 10)           327690      ['flatten_2[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 409,678\n",
            "Trainable params: 409,678\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate model"
      ],
      "metadata": {
        "id": "Rkx9nThzXQkP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "HEHJ0qXiDd0i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eb1e762-4102-4db4-baa6-6d89787f9dee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 5ms/step - loss: 2.0350 - acc: 0.3621\n",
            "Test loss: 2.0349903106689453\n",
            "Test accuracy: 0.3621000051498413\n"
          ]
        }
      ],
      "source": [
        "# Save model and weights\n",
        "#if not os.path.isdir(save_dir):\n",
        "#    os.makedirs(save_dir)\n",
        "#model_path = os.path.join(save_dir, model_name)\n",
        "#model.save(model_path)\n",
        "#print('Saved trained model at %s ' % model_path)\n",
        "\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ]
    }
  ]
}